## 模型优化
- Graph: 在不改变模型最终数学逻辑的前提下，通过修改计算流程的“蓝图”（计算图）来提升效率，例如将多个小步骤合并成一个大步骤（算子融合，QKVParallelLinear）。
- Op:专注于将每一个独立的计算步骤（算子，如卷积、加法）本身，用针对特定硬件（如CPU或GPU）最高效的代码来实现（conv2d ➡ matmul）。
- Runtime: 在模型真正执行推理时，通过智能地调度计算任务、管理内存等方式，最大限度地压榨硬件性能，减少等待和浪费（PageAttention, Continuous Batching）。

## AI推理加速（算力，I/O，访存，通信）
- 计算效率：取决于硬件算力，以及计算的持续而不被打断
- 访存效率：取决于访存延迟和带宽
- 计算与访存相重叠
- 计算与通信相重叠

## 模型大小评估指标
1. 计算量 (Computational Cost)
定义：计算量反映了模型对硬件计算单元的需求。计算量的单位通常是 OPs (Operations) 或 FLOPs (Floating Point Operations)，即浮点运算次数。在深度学习中，最常用的数据格式为 float32，因此 float32 类型下的计算量常被写作 FLOPs。模型的整体计算量等于模型中每个算子的计算量之和。

例子：对两个 shape 为 (N, C, H, W) 的 float32 tensor 进行 add 操作，其计算量为 N×C×H×W FLOPs。

2. 参数量 (Number of Parameters)
定义：模型中所有需要学习的参数的总和，它直接反映了模型占用的磁盘空间大小。

说明：对于 CNN（卷积神经网络）而言，参数主要由 Conv (卷积) 层和 FC (全连接) 层的 Weight (权重) 构成。其他算子（如 BatchNorm、激活函数等）虽然也可能有参数，但与前者相比通常占比较小。

3. 访存量 (Memory Access)
定义：指模型计算时所需访问内存/显存的字节大小，它反映了模型对内存/显存带宽的需求。访存量单位为 Bytes，表示模型计算到底需要读取/取多少 Bytes 的数据。

    例子：对两个 shape 为 (N, C, H, W) 的 float32 tensor 进行 add 操作，访存量为 (2+1)×N×C×H×W×sizeof(float32) bytes。其中“2”代表读取两个输入张量，“1”代表写入一个输出张量。

4. (峰值)内存占用 (Peak Memory Usage)
- 定义：指模型跑起来的时候（训练或推理）所占用的内存/显存大小。峰值内存占用，特指在运行过程中的内存/显存占用的峰值。注意：内存占用 ≠ 访存量。

- 峰值内存占用的特征描述：

  - 动态性 (Dynamic)：峰值内存是一个动态指标，它在模型单次迭代（iteration）的执行过程中是不断变化的。如上图所示，内存在前向传播（forward）阶段通常会持续增长，在反向传播（backward）阶段会因为部分中间变量被释放而有所波动和下降。

  - 综合性 (Comprehensive)：它不仅包括模型本身的参数（权重），还包括训练或推理过程中产生的中间激活值（feature maps）、计算出的梯度（在训练时），以及优化器（Optimizer）自身的状态（如动量信息）等。

  - 训练远高于推理：通常情况下，模型训练时的峰值内存占用远大于推理时。这是因为训练需要保存前向传播过程中的所有中间激活值，以便在反向传播时计算梯度，同时还要存储梯度本身和优化器的状态。而推理过程通常只需要保留前一层的输出供后一层使用，可以做到“阅后即焚”。

  - 非简单叠加：峰值内存并不是所有张量（参数、激活值、梯度）大小的简单相加。现代深度学习框架有高效的内存管理机制，会复用内存空间。峰值内存反映的是在某一特定时刻，同时存在于内存/显存中的所有张量的总大小的最大值。